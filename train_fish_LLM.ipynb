{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextGenerationPipeline, AutoModelForCausalLM, LlamaTokenizerFast, AutoModelForSequenceClassification\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\", trust_remote_code=True, num_labels=105).cuda()\n",
    "tokenizer: LlamaTokenizerFast = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from accelerate import Accelerator\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        # output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        # output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        # output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "def get_special_tokens_dict(tokenizer):\n",
    "    special_tokens_dict = dict()\n",
    "    if tokenizer.pad_token is None:\n",
    "        special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "    if tokenizer.eos_token is None:\n",
    "        special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "    if tokenizer.bos_token is None:\n",
    "        special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "    if tokenizer.unk_token is None:\n",
    "        special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "    return special_tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = get_special_tokens_dict(tokenizer)\n",
    "smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict=special_tokens_dict,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.columns)\n",
    "# print(len(data))\n",
    "# print(data.iloc[0])\n",
    "# for d in data.iloc[0]:\n",
    "#     print(type(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # 假设data是一个pandas DataFrame，并且已经加载了相应的数据\n",
    "# # 统计Label列中不同标签的数量\n",
    "# label_counts = data['Label'].value_counts()\n",
    "# # 获取不同标签的列表\n",
    "# unique_labels = label_counts.index.tolist()\n",
    "# print(unique_labels)\n",
    "# print(len(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # 假设 data 是一个已经存在的 pandas DataFrame\n",
    "\n",
    "# # 检查 'TextData' 列是否全为非空字符串\n",
    "# all_strings = data['TextData'].apply(lambda x: isinstance(x, str) and x.strip() != '').all()\n",
    "\n",
    "# # 检查 'LabelIndex' 列是否都为 int\n",
    "# all_ints = data['LabelIndex'].apply(lambda x: isinstance(x, int) and not pd.isnull(x)).all()\n",
    "\n",
    "# # 打印结果\n",
    "# print(\"'TextData' 是否全为非空字符串:\", all_strings)\n",
    "# print(\"'LabelIndex' 是否都为 int:\", all_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification, EvalPrediction\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    logits, labels = torch.tensor(logits), torch.tensor(labels)\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions, zero_division=0, average=\"weighted\")\n",
    "    return {'accuracy': accuracy, 'recall': recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/opt/tiger/network/results',          # 输出目录\n",
    "    num_train_epochs=50,              # 训练轮数\n",
    "    per_device_train_batch_size=32,  # 训练时每个设备的batch size\n",
    "    per_device_eval_batch_size=800,   # 评估时的batch size\n",
    "    warmup_steps=500,                # 预热步数\n",
    "    weight_decay=0.01,               # 权重衰减\n",
    "    logging_dir='./logs',            # 日志目录\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-5,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    save_total_limit=3\n",
    ")\n",
    "\n",
    "# 定义数据处理器\n",
    "class CustomDataset:\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),  # Remove batch dimension\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Email Text', 'Email Type'], dtype='object')\n",
      "re : 6 . 1100 , disc : uniformitarianism , re : 1086 ; sex / lang dick hudson 's observations on us use of 's on ' but not 'd aughter ' as a vocative are very thought-provoking , but i am not sure that it is fair to attribute this to \" sons \" being \" treated like senior relatives \" . for one thing , we do n't normally use ' brother ' in this way any more than we do 'd aughter ' , and it is hard to imagine a natural class comprising senior relatives and 's on ' but excluding ' brother ' . for another , there seem to me to be differences here . if i am not imagining a distinction that is not there , it seems to me that the senior relative terms are used in a wider variety of contexts , e . g . , calling out from a distance to get someone 's attention , and hence at the beginning of an utterance , whereas 's on ' seems more natural in utterances like ' yes , son ' , ' hand me that , son ' than in ones like ' son ! ' or ' son , help me ! ' ( although perhaps these latter ones are not completely impossible ) . alexis mr\n",
      "18634\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"/opt/tiger/network_attck/Phishing_Email.csv\")\n",
    "print(data.columns)\n",
    "# 使用 apply 函数和 lambda 表达式对每一行进行处理\n",
    "# join 方法将列值通过空格连接成一个字符串\n",
    "# data['combined_columns'] = data.drop(columns=['Email Type']).apply(lambda row: ' '.join(row.astype(str)), axis=1)\n",
    "\n",
    "# 查看结果\n",
    "print(data['Email Text'][0])\n",
    "data = data[data['Email Text'].apply(lambda x: isinstance(x, str))]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Safe Email', 'Phishing Email'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 将Label列转换为整数索引\n",
    "data['LabelIndex'], unique_labels = pd.factorize(data['Email Type'])\n",
    "print(unique_labels)\n",
    "# data.to_csv(\"processed_ddos_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据集\n",
    "texts = data['Email Text'].tolist()  # 用实际的文本列名替换'YourTextColumn'\n",
    "labels = data['LabelIndex'].tolist()\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# 创建数据集实例\n",
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = CustomDataset(test_texts, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 保存 train_dataset\n",
    "torch.save({\n",
    "    'texts': train_texts,\n",
    "    'labels': train_labels,\n",
    "}, '/opt/tiger/network_attck/fish_train_dataset.pt')\n",
    "\n",
    "# 保存 test_dataset\n",
    "torch.save({\n",
    "    'texts': test_texts,\n",
    "    'labels': test_labels,\n",
    "}, '/opt/tiger/network_attck/fish_test_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = torch.load(\"train_dataset.pt\"), torch.load(\"test_dataset.pt\")\n",
    "train_texts, train_labels = train[\"texts\"], train[\"labels\"]\n",
    "test_texts, test_labels = test[\"texts\"], test[\"labels\"]\n",
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = CustomDataset(test_texts, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.143, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# 初始化Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,  # 传入测试数据集\n",
    "    compute_metrics=compute_metrics,  # 指定计算指标的函数\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n192-024-074:365874:365874 [0] NCCL INFO cudaDriverVersion 12010\n",
      "n192-024-074:365874:365874 [0] NCCL INFO NCCL_SOCKET_FAMILY set by environment to AF_INET6\n",
      "n192-024-074:365874:365874 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\n",
      "n192-024-074:365874:365874 [0] NCCL INFO Bootstrap : Using eth0:fdbd:dc61:7:34::74<0>\n",
      "n192-024-074:365874:365874 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation\n",
      "NCCL version 2.20.5+cuda12.4\n",
      "n192-024-074:365874:366392 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.\n",
      "n192-024-074:365874:366392 [1] NCCL INFO NCCL_SOCKET_FAMILY set by environment to AF_INET6\n",
      "n192-024-074:365874:366392 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\n",
      "n192-024-074:365874:366392 [1] NCCL INFO NCCL_IB_HCA set to mlx5_2:1\n",
      "n192-024-074:365874:366392 [1] NCCL INFO NET/IB : Using [0]mlx5_2:1/RoCE [RO]; OOB eth0:fdbd:dc61:7:34::74<0>\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Using non-device net plugin version 0\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Using network IB\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Using non-device net plugin version 0\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Using network IB\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Using non-device net plugin version 0\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Using network IB\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Using non-device net plugin version 0\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Using network IB\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Using non-device net plugin version 0\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Using network IB\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Using non-device net plugin version 0\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Using network IB\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Using non-device net plugin version 0\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Using network IB\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Using non-device net plugin version 0\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Using network IB\n",
      "n192-024-074:365874:366392 [1] NCCL INFO comm 0x19f75870 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 1b000 commId 0xc8eb47b1a25701ce - Init START\n",
      "n192-024-074:365874:366397 [6] NCCL INFO comm 0x19f95230 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId b1000 commId 0xc8eb47b1a25701ce - Init START\n",
      "n192-024-074:365874:366398 [7] NCCL INFO comm 0x19f9b740 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId b2000 commId 0xc8eb47b1a25701ce - Init START\n",
      "n192-024-074:365874:366396 [5] NCCL INFO comm 0x19f8ed20 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 89000 commId 0xc8eb47b1a25701ce - Init START\n",
      "n192-024-074:365874:366391 [0] NCCL INFO comm 0x19f70260 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 1a000 commId 0xc8eb47b1a25701ce - Init START\n",
      "n192-024-074:365874:366393 [2] NCCL INFO comm 0x19f7bd80 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 3d000 commId 0xc8eb47b1a25701ce - Init START\n",
      "n192-024-074:365874:366395 [4] NCCL INFO comm 0x19f88810 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 88000 commId 0xc8eb47b1a25701ce - Init START\n",
      "n192-024-074:365874:366394 [3] NCCL INFO comm 0x19f82300 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 3e000 commId 0xc8eb47b1a25701ce - Init START\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\n",
      "n192-024-074:365874:366396 [5] NCCL INFO NVLS multicast support is not available on dev 5\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\n",
      "n192-024-074:365874:366391 [0] NCCL INFO NVLS multicast support is not available on dev 0\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\n",
      "n192-024-074:365874:366393 [2] NCCL INFO NVLS multicast support is not available on dev 2\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\n",
      "n192-024-074:365874:366397 [6] NCCL INFO NVLS multicast support is not available on dev 6\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\n",
      "n192-024-074:365874:366394 [3] NCCL INFO NVLS multicast support is not available on dev 3\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\n",
      "n192-024-074:365874:366398 [7] NCCL INFO NVLS multicast support is not available on dev 7\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\n",
      "n192-024-074:365874:366392 [1] NCCL INFO NVLS multicast support is not available on dev 1\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\n",
      "n192-024-074:365874:366395 [4] NCCL INFO NVLS multicast support is not available on dev 4\n",
      "n192-024-074:365874:366395 [4] NCCL INFO comm 0x19f88810 rank 4 nRanks 8 nNodes 1 localRanks 8 localRank 4 MNNVL 0\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->6 [2] 6/-1/-1->4->7 [3] 6/-1/-1->4->5 [4] 3/-1/-1->4->5 [5] 7/-1/-1->4->-1 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->6 [8] 6/-1/-1->4->7 [9] 6/-1/-1->4->5 [10] 3/-1/-1->4->5 [11] 7/-1/-1->4->-1\n",
      "n192-024-074:365874:366395 [4] NCCL INFO P2P Chunksize set to 524288\n",
      "n192-024-074:365874:366394 [3] NCCL INFO comm 0x19f82300 rank 3 nRanks 8 nNodes 1 localRanks 8 localRank 3 MNNVL 0\n",
      "n192-024-074:365874:366393 [2] NCCL INFO comm 0x19f7bd80 rank 2 nRanks 8 nNodes 1 localRanks 8 localRank 2 MNNVL 0\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 1/-1/-1->3->2 [2] -1/-1/-1->3->1 [3] 2/-1/-1->3->1 [4] 2/-1/-1->3->4 [5] 1/-1/-1->3->0 [6] 4/-1/-1->3->2 [7] 1/-1/-1->3->2 [8] -1/-1/-1->3->1 [9] 2/-1/-1->3->1 [10] 2/-1/-1->3->4 [11] 1/-1/-1->3->0\n",
      "n192-024-074:365874:366392 [1] NCCL INFO comm 0x19f75870 rank 1 nRanks 8 nNodes 1 localRanks 8 localRank 1 MNNVL 0\n",
      "n192-024-074:365874:366394 [3] NCCL INFO P2P Chunksize set to 524288\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->0 [2] 5/-1/-1->2->0 [3] -1/-1/-1->2->3 [4] 1/-1/-1->2->3 [5] 0/-1/-1->2->5 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->0 [8] 5/-1/-1->2->0 [9] -1/-1/-1->2->3 [10] 1/-1/-1->2->3 [11] 0/-1/-1->2->5\n",
      "n192-024-074:365874:366398 [7] NCCL INFO comm 0x19f9b740 rank 7 nRanks 8 nNodes 1 localRanks 8 localRank 7 MNNVL 0\n",
      "n192-024-074:365874:366397 [6] NCCL INFO comm 0x19f95230 rank 6 nRanks 8 nNodes 1 localRanks 8 localRank 6 MNNVL 0\n",
      "n192-024-074:365874:366391 [0] NCCL INFO comm 0x19f70260 rank 0 nRanks 8 nNodes 1 localRanks 8 localRank 0 MNNVL 0\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 6/-1/-1->1->3 [2] 3/-1/-1->1->6 [3] 3/-1/-1->1->6 [4] 6/-1/-1->1->2 [5] -1/-1/-1->1->3 [6] 2/-1/-1->1->0 [7] 6/-1/-1->1->3 [8] 3/-1/-1->1->6 [9] 3/-1/-1->1->6 [10] 6/-1/-1->1->2 [11] -1/-1/-1->1->3\n",
      "n192-024-074:365874:366392 [1] NCCL INFO P2P Chunksize set to 524288\n",
      "n192-024-074:365874:366393 [2] NCCL INFO P2P Chunksize set to 524288\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 4/-1/-1->6->1 [2] 1/-1/-1->6->4 [3] 1/-1/-1->6->4 [4] -1/-1/-1->6->1 [5] 5/-1/-1->6->7 [6] 7/-1/-1->6->5 [7] 4/-1/-1->6->1 [8] 1/-1/-1->6->4 [9] 1/-1/-1->6->4 [10] -1/-1/-1->6->1 [11] 5/-1/-1->6->7\n",
      "n192-024-074:365874:366397 [6] NCCL INFO P2P Chunksize set to 524288\n",
      "n192-024-074:365874:366396 [5] NCCL INFO comm 0x19f8ed20 rank 5 nRanks 8 nNodes 1 localRanks 8 localRank 5 MNNVL 0\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 00/12 :    0   1   2   3   4   5   6   7\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 01/12 :    0   2   3   1   6   4   5   7\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 02/12 :    0   2   5   7   4   6   1   3\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 03/12 :    0   7   5   4   6   1   3   2\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 7/-1/-1->5->4 [2] 7/-1/-1->5->2 [3] 4/-1/-1->5->7 [4] 4/-1/-1->5->7 [5] 2/-1/-1->5->6 [6] 6/-1/-1->5->4 [7] 7/-1/-1->5->4 [8] 7/-1/-1->5->2 [9] 4/-1/-1->5->7 [10] 4/-1/-1->5->7 [11] 2/-1/-1->5->6\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 04/12 :    0   7   6   5   4   3   2   1\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 05/12 :    0   3   1   6   4   7   5   2\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 06/12 :    0   1   2   3   4   5   6   7\n",
      "n192-024-074:365874:366396 [5] NCCL INFO P2P Chunksize set to 524288\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->5 [2] 4/-1/-1->7->5 [3] 5/-1/-1->7->0 [4] 5/-1/-1->7->0 [5] 6/-1/-1->7->4 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->5 [8] 4/-1/-1->7->5 [9] 5/-1/-1->7->0 [10] 5/-1/-1->7->0 [11] 6/-1/-1->7->4\n",
      "n192-024-074:365874:366398 [7] NCCL INFO P2P Chunksize set to 524288\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 07/12 :    0   2   3   1   6   4   5   7\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 08/12 :    0   2   5   7   4   6   1   3\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 09/12 :    0   7   5   4   6   1   3   2\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 10/12 :    0   7   6   5   4   3   2   1\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 11/12 :    0   3   1   6   4   7   5   2\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 2/-1/-1->0->-1 [2] 2/-1/-1->0->-1 [3] 7/-1/-1->0->-1 [4] 7/-1/-1->0->-1 [5] 3/-1/-1->0->2 [6] 1/-1/-1->0->-1 [7] 2/-1/-1->0->-1 [8] 2/-1/-1->0->-1 [9] 7/-1/-1->0->-1 [10] 7/-1/-1->0->-1 [11] 3/-1/-1->0->2\n",
      "n192-024-074:365874:366391 [0] NCCL INFO P2P Chunksize set to 524288\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 00/0 : 7[7] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 06/0 : 5[5] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 02/0 : 4[4] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 01/0 : 5[5] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 02/0 : 1[1] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 01/0 : 0[0] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 03/0 : 4[4] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 02/0 : 5[5] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 03/0 : 1[1] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 02/0 : 0[0] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 08/0 : 4[4] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 07/0 : 5[5] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 08/0 : 1[1] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 07/0 : 0[0] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 09/0 : 4[4] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 08/0 : 5[5] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 09/0 : 1[1] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 08/0 : 0[0] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 05/0 : 4[4] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 02/0 : 6[6] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 02/0 : 2[2] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 05/0 : 0[0] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 11/0 : 4[4] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 03/0 : 6[6] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 11/0 : 0[0] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 08/0 : 2[2] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 08/0 : 6[6] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 02/0 : 7[7] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 02/0 : 3[3] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 05/0 : 5[5] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 09/0 : 6[6] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 08/0 : 7[7] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 08/0 : 3[3] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 11/0 : 5[5] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 03/0 : 7[7] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 01/0 : 1[1] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 01/0 : 3[3] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 05/0 : 7[7] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 03/0 : 2[2] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 05/0 : 1[1] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 05/0 : 3[3] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 09/0 : 7[7] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 05/0 : 2[2] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 07/0 : 1[1] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 07/0 : 3[3] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 11/0 : 7[7] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 09/0 : 2[2] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 11/0 : 1[1] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 11/0 : 3[3] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 11/0 : 2[2] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 01/0 : 6[6] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 05/0 : 6[6] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 07/0 : 6[6] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 03/0 : 0[0] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 11/0 : 6[6] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 04/0 : 0[0] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 09/0 : 0[0] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 10/0 : 0[0] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Connected all rings\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Connected all rings\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Connected all rings\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Connected all rings\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Connected all rings\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Connected all rings\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Connected all rings\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Connected all rings\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 04/0 : 3[3] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 05/0 : 0[0] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 01/0 : 1[1] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 11/0 : 0[0] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 05/0 : 1[1] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 07/0 : 1[1] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 05/0 : 2[2] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 01/0 : 4[4] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 11/0 : 1[1] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 03/0 : 5[5] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 11/0 : 2[2] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 07/0 : 4[4] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 04/0 : 5[5] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 05/0 : 3[3] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 02/0 : 4[4] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 01/0 : 6[6] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 09/0 : 5[5] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 11/0 : 3[3] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 08/0 : 4[4] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 04/0 : 6[6] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 10/0 : 5[5] -> 7[7] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 02/0 : 3[3] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 07/0 : 6[6] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 03/0 : 3[3] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 10/0 : 6[6] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 08/0 : 3[3] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 02/0 : 1[1] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 05/0 : 7[7] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 02/0 : 5[5] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 09/0 : 3[3] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 03/0 : 1[1] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 11/0 : 7[7] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 08/0 : 5[5] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 04/0 : 1[1] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 01/0 : 7[7] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 08/0 : 1[1] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 01/0 : 2[2] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 02/0 : 7[7] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 09/0 : 1[1] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 02/0 : 2[2] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 04/0 : 7[7] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 10/0 : 1[1] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 07/0 : 2[2] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 07/0 : 7[7] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 02/0 : 6[6] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 08/0 : 2[2] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 08/0 : 7[7] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 03/0 : 6[6] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 10/0 : 7[7] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 08/0 : 6[6] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 09/0 : 6[6] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/direct pointer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/direct pointer\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Connected all trees\n",
      "n192-024-074:365874:366391 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\n",
      "n192-024-074:365874:366391 [0] NCCL INFO 12 coll channels, 0 collnet channels, 0 nvls channels, 16 p2p channels, 2 p2p channels per peer\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Connected all trees\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Connected all trees\n",
      "n192-024-074:365874:366393 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\n",
      "n192-024-074:365874:366393 [2] NCCL INFO 12 coll channels, 0 collnet channels, 0 nvls channels, 16 p2p channels, 2 p2p channels per peer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Connected all trees\n",
      "n192-024-074:365874:366398 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\n",
      "n192-024-074:365874:366398 [7] NCCL INFO 12 coll channels, 0 collnet channels, 0 nvls channels, 16 p2p channels, 2 p2p channels per peer\n",
      "n192-024-074:365874:366396 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 04/1 : 2[2] -> 4[4] via P2P/indirect/5[5]\n",
      "n192-024-074:365874:366396 [5] NCCL INFO 12 coll channels, 0 collnet channels, 0 nvls channels, 16 p2p channels, 2 p2p channels per peer\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Connected all trees\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 04/1 : 7[7] -> 1[1] via P2P/indirect/6[6]\n",
      "n192-024-074:365874:366395 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\n",
      "n192-024-074:365874:366395 [4] NCCL INFO 12 coll channels, 0 collnet channels, 0 nvls channels, 16 p2p channels, 2 p2p channels per peer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Connected all trees\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Connected all trees\n",
      "n192-024-074:365874:366397 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Connected all trees\n",
      "n192-024-074:365874:366397 [6] NCCL INFO 12 coll channels, 0 collnet channels, 0 nvls channels, 16 p2p channels, 2 p2p channels per peer\n",
      "n192-024-074:365874:366392 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\n",
      "n192-024-074:365874:366392 [1] NCCL INFO 12 coll channels, 0 collnet channels, 0 nvls channels, 16 p2p channels, 2 p2p channels per peer\n",
      "n192-024-074:365874:366394 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\n",
      "n192-024-074:365874:366394 [3] NCCL INFO 12 coll channels, 0 collnet channels, 0 nvls channels, 16 p2p channels, 2 p2p channels per peer\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 04/1 : 6[6] -> 0[0] via P2P/indirect/7[7]\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 04/1 : 3[3] -> 5[5] via P2P/indirect/4[4]\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 05/1 : 7[7] -> 1[1] via P2P/indirect/6[6]\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 05/1 : 2[2] -> 4[4] via P2P/indirect/5[5]\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 05/1 : 6[6] -> 0[0] via P2P/indirect/7[7]\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 05/1 : 3[3] -> 5[5] via P2P/indirect/4[4]\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 12/1 : 7[7] -> 2[2] via P2P/indirect/0[0]\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 12/1 : 1[1] -> 4[4] via P2P/indirect/6[6]\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 12/1 : 5[5] -> 0[0] via P2P/indirect/7[7]\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 12/1 : 3[3] -> 6[6] via P2P/indirect/1[1]\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 13/1 : 1[1] -> 4[4] via P2P/indirect/6[6]\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 13/1 : 7[7] -> 2[2] via P2P/indirect/0[0]\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 13/1 : 3[3] -> 6[6] via P2P/indirect/1[1]\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 13/1 : 5[5] -> 0[0] via P2P/indirect/7[7]\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 02/1 : 6[6] -> 2[2] via P2P/indirect/5[5]\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 02/1 : 3[3] -> 7[7] via P2P/indirect/0[0]\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 02/1 : 4[4] -> 0[0] via P2P/indirect/7[7]\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 03/1 : 6[6] -> 2[2] via P2P/indirect/5[5]\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 02/1 : 2[2] -> 6[6] via P2P/indirect/1[1]\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 02/1 : 0[0] -> 4[4] via P2P/indirect/3[3]\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 03/1 : 4[4] -> 0[0] via P2P/indirect/7[7]\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 02/1 : 7[7] -> 3[3] via P2P/indirect/0[0]\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 02/1 : 5[5] -> 1[1] via P2P/indirect/6[6]\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 03/1 : 2[2] -> 6[6] via P2P/indirect/1[1]\n",
      "n192-024-074:365874:366394 [3] NCCL INFO Channel 03/1 : 3[3] -> 7[7] via P2P/indirect/0[0]\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 02/1 : 1[1] -> 5[5] via P2P/indirect/6[6]\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 03/1 : 0[0] -> 4[4] via P2P/indirect/3[3]\n",
      "n192-024-074:365874:366398 [7] NCCL INFO Channel 03/1 : 7[7] -> 3[3] via P2P/indirect/0[0]\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 10/1 : 2[2] -> 7[7] via P2P/indirect/0[0]\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 10/1 : 6[6] -> 3[3] via P2P/indirect/1[1]\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 03/1 : 5[5] -> 1[1] via P2P/indirect/6[6]\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 10/1 : 4[4] -> 1[1] via P2P/indirect/6[6]\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 10/1 : 0[0] -> 5[5] via P2P/indirect/7[7]\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 03/1 : 1[1] -> 5[5] via P2P/indirect/6[6]\n",
      "n192-024-074:365874:366397 [6] NCCL INFO Channel 11/1 : 6[6] -> 3[3] via P2P/indirect/1[1]\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 11/1 : 0[0] -> 5[5] via P2P/indirect/7[7]\n",
      "n192-024-074:365874:366393 [2] NCCL INFO Channel 11/1 : 2[2] -> 7[7] via P2P/indirect/0[0]\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 11/1 : 4[4] -> 1[1] via P2P/indirect/6[6]\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 06/1 : 4[4] -> 2[2] via P2P/indirect/3[3]\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 06/1 : 1[1] -> 7[7] via P2P/indirect/0[0]\n",
      "n192-024-074:365874:366395 [4] NCCL INFO Channel 07/1 : 4[4] -> 2[2] via P2P/indirect/3[3]\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 06/1 : 0[0] -> 6[6] via P2P/indirect/1[1]\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 06/1 : 5[5] -> 3[3] via P2P/indirect/2[2]\n",
      "n192-024-074:365874:366392 [1] NCCL INFO Channel 07/1 : 1[1] -> 7[7] via P2P/indirect/0[0]\n",
      "n192-024-074:365874:366396 [5] NCCL INFO Channel 07/1 : 5[5] -> 3[3] via P2P/indirect/2[2]\n",
      "n192-024-074:365874:366391 [0] NCCL INFO Channel 07/1 : 0[0] -> 6[6] via P2P/indirect/1[1]\n",
      "n192-024-074:365874:366391 [0] NCCL INFO comm 0x19f70260 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 1a000 commId 0xc8eb47b1a25701ce - Init COMPLETE\n",
      "n192-024-074:365874:366398 [7] NCCL INFO comm 0x19f9b740 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId b2000 commId 0xc8eb47b1a25701ce - Init COMPLETE\n",
      "n192-024-074:365874:366395 [4] NCCL INFO comm 0x19f88810 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 88000 commId 0xc8eb47b1a25701ce - Init COMPLETE\n",
      "n192-024-074:365874:366396 [5] NCCL INFO comm 0x19f8ed20 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 89000 commId 0xc8eb47b1a25701ce - Init COMPLETE\n",
      "n192-024-074:365874:366394 [3] NCCL INFO comm 0x19f82300 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 3e000 commId 0xc8eb47b1a25701ce - Init COMPLETE\n",
      "n192-024-074:365874:366397 [6] NCCL INFO comm 0x19f95230 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId b1000 commId 0xc8eb47b1a25701ce - Init COMPLETE\n",
      "n192-024-074:365874:366393 [2] NCCL INFO comm 0x19f7bd80 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 3d000 commId 0xc8eb47b1a25701ce - Init COMPLETE\n",
      "n192-024-074:365874:366392 [1] NCCL INFO comm 0x19f75870 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 1b000 commId 0xc8eb47b1a25701ce - Init COMPLETE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='846' max='2950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 846/2950 23:33 < 58:43, 0.60 it/s, Epoch 14.32/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.861400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.789400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.711200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.590100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.421400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>4.248300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.057600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.902700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.713700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.542600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>3.172500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.994400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.785600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.598200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.427500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.246700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.094800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.907600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.757600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.578200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.418600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.258200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.083300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.930900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.800600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.675700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.553800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.449700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.382100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.328400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.305600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.256700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.242900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.204400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.182900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.142600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.143400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.129700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.108100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.092100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.083600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.080700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.058400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.058700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.054100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.043700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.044100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.043500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.039200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.035900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.039200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.043200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.032800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.030700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.030500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.029700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.065981</td>\n",
       "      <td>0.982291</td>\n",
       "      <td>0.982291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1860\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1861\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1862\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1863\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1864\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2165\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2162\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   2164\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 2165\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2166\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   2168\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/accelerate/data_loader.py:462\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     current_batch \u001b[39m=\u001b[39m send_to_device(current_batch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 462\u001b[0m next_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(dataloader_iter)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m batch_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_batches:\n\u001b[1;32m    464\u001b[0m     \u001b[39myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     33\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexts[idx]\n\u001b[1;32m     34\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m---> 35\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(),  \u001b[38;5;66;03m# Remove batch dimension\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(),\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(label, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     40\u001b[0m }\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2858\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2856\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2857\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2858\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2859\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2860\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2964\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2944\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2945\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2946\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2961\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2962\u001b[0m     )\n\u001b[1;32m   2963\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2964\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m   2965\u001b[0m         text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2966\u001b[0m         text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2967\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2968\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2969\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2970\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2971\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2972\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2973\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2974\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2975\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2976\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2977\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2978\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2979\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2980\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2981\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2982\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2983\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3037\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3028\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3029\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   3030\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3034\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3035\u001b[0m )\n\u001b[0;32m-> 3037\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   3038\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   3039\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   3040\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   3041\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   3042\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   3043\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   3044\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   3045\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   3046\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   3047\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   3048\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   3049\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   3050\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   3051\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   3052\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   3053\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   3054\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   3055\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3056\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:576\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[1;32m    555\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    556\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    574\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    575\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[0;32m--> 576\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m    577\u001b[0m         batched_input,\n\u001b[1;32m    578\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    579\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    580\u001b[0m         padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    581\u001b[0m         truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    582\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    583\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    584\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    585\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    586\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    587\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    588\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    589\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    590\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m    591\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    592\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    593\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    594\u001b[0m     )\n\u001b[1;32m    596\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:504\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    497\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[1;32m    498\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    502\u001b[0m )\n\u001b[0;32m--> 504\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode_batch(\n\u001b[1;32m    505\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    506\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    507\u001b[0m     is_pretokenized\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    508\u001b[0m )\n\u001b[1;32m    510\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    516\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[1;32m    517\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[1;32m    518\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[1;32m    528\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.06598091870546341,\n",
       " 'eval_accuracy': 0.9822913871746713,\n",
       " 'eval_recall': 0.9822913871746713}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
